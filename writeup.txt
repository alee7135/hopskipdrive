1. What are your main recommendations based on the data? 
I have build a multistep prototype solution to solve this ride/trip Boosting problem. The first step is to feed in a given trip_id along with all meta data about that trip to predict the expected claim time elasticity (i.e. the ratio of % change in claim time to % change in boosting costs). The expected elasticity will be negative because the more we increase boost cost, the more we decrease claim time. Next, I simulated across multiple boosting costs in model.ipynb, using the elasticity we predicted to compute a new predicted claim time. To find the optimal solution, I defer to the business to create an objective function that captures the tradeoff between boost cost and claim time. In the model.ipynb, you can see the simulated scenarios along with a default objective function graph to locate the optimal boosting cost. For example, given -0.5 elasticity trip and base_boost=$5 and baseline_time_to_claim=24hrs and 50/50 tradeoff, I would recommend to increase the boosting cost to $9.2. Comparatively, if we look at a more elastic -1.6, we only need to boost to ~$6 to achieve same objective result because drivers are more responsive per % increase in boost. I would also recommend a procedure where the business has a max-budget and other constraints to determine the objective function. Following, we can logically build a dynamic pricing algorithm to make these decisions and A/B test it. This way, we are using data to determine how much we boost, making sure we don't boost too much or too little but just right.

2. What other data might you want or need to test assumptions or improve model performance? 
- meta data about the drivers and customers 
- data about driver resources in given location and time 
- validation of elasticities

3. How confident are you that the assumptions you made will generate improved outcomes? 
The assumption is that different trip_id have different elasticity behavior in terms of how boosting will affect claim times. We want as much as possible for the optimizaion to work effectively. I think this is a valid assumption but it will require further exploration of elasticities as it is key in this approach. We should explore the linearity of the elasticity curve as it may vary based on boost cost range which we can then try non-linear models to estimate the elasticity. If we can have valid elasticities, then I am confident that we can use them for dynamically "pricing" the boostings based on any given trip_id at given time and location.

4. Write up a short description of any toolkits or processes that you believe would improve the workflow, the process, or the model performance. It is acceptable if you see none. 
The model requires more validation and testing obviously, more iteration, more experimentation, more input from domain-experts.

5. Assume that this model is performant. What steps would you take to deliver the predictions or results routinely and at scale? 
If it is performant, I would would work with engineers to build a production ready optimization engine and work with product managers to A/B test the new algorithm. To proof it, I would need to write unit tests to test all potential corner cases and errors. I would develop separate ETL pipeline, Training pipeline, Inference, Optimization pipelines in appropriate technologies. I would build in automated retraining and model monitoring for drift and create alerts for unusual anomalies. Finally, I would write clear and detailed documentation for how the algorithm works. 

6. Please push the code and write up files (text, markdown, quarto, html) to a public github repo that we can access. Share the final repo link with bweiner@hopskipdrive.com along with any documentation. Candidates who show promising results on this may be asked to participate in a panel interview. Reminder: Even though the data are synthetic, do not push data, secrets, or credentials to Github.